package Libraries.Compute.Statistics.Tests

use Libraries.Compute.Statistics.DataFrame
use Libraries.Compute.Statistics.DataFrameColumn
use Libraries.Compute.Statistics.Inputs.ColumnInput
use Libraries.Compute.Statistics.Distributions.HeavyTailNormalDistribution
use Libraries.Compute.Statistics.Distributions.StudentizedRangeDistribution
use Libraries.Compute.Math
use Libraries.Containers.Array
use Libraries.Compute.Statistics.Calculations.Variance
use Libraries.Compute.Statistics.Reporting.CompareMeansResult
use Libraries.Containers.HashTable
use Libraries.Compute.Statistics.Distributions.ClassificationDistribution
use Libraries.Compute.Statistics.Distributions.NormalDistribution
use Libraries.Compute.Statistics.Calculations.Summarize

/*
This class implements several post hoc analysis tests, which are intended to be used 
after a significant CompareMeans test with three or more samples. This class can take two 
approaches. The first which is the default, called the 'fitted' approach, will use the fiited result of the prior 
CompareMeans test to calculate the standard error and adjustments to correct for familywise error. 
The formal names of tests included in fitted approach are as follows: Tukey's HSD, Tukey-Kramer, Games-Howell, 
Emmeans-Bonferroni, Nemenyi, Dunn-Bonferroni, and Conover-Iman-Bonferroni. The second approach, 
called the 'unfitted' approach, will not use the fitted result of prior CompareMeans test, instead it will 
run individual two-sample CompareMeans tests and then correct for familywise error using Bonferroni.
The formal names of tests included in the unfitted approach are as follows: t-test, Welch's t-test, paired t-test,
paired Wilcoxon test, and Mann-Whitney-Wilcoxon test. There are two types of corrections as well, the first is 'lenient', 
which is an inherent correction applied in some tests, the second is 'strict' which refers to any test that applies the
Bonferroni correction to the p-values. See the INFORMATION comment block at the bottom of this class for more information
about each test.
For more information: https://en.wikipedia.org/wiki/Post_hoc_analysis
    

Attribute: Author Andreas Stefik, Hannah Stabler
Attribute: Example

use Libraries.Compute.Statistics.DataFrame
use Libraries.Compute.Statistics.Tests.CompareMeansPairwise

DataFrame frame
frame:Load("Data/Data.csv")

// this example will show a two-factor between subjects design

// make a design for the test to follow 
ExperimentalDesign design

// select the name of a column in your frame as a first factor (independent variable)
design:AddBetweenSubjectsFactor("Age")      

// select the name of a column in your frame as a second factor (independent variable)
design:AddBetweenSubjectsFactor("Group")    

// select the name of a column in your frame as the dependent variable
design:AddDependentVariable("Response")    

// tell the frame to use this design and run a compare means pairwise test
CompareMeansPairwise pairwise = frame:CompareMeansPairwise(design)

// output the pairwise summary, a list of adjusted p-values for every comparison
output pairwise:GetPairwiseSummary()

// if you only want a specific set of comparisons based on a certain factor or interaction
output pairwise:GetPairwiseSummary("Age:Group")
*/
class CompareMeansPairwise is CompareMeans
    /* The distribution used to calculate the p-value in emmeans and cononvers test.*/
    private HeavyTailNormalDistribution tDistribution

    /* The distribution used to calculate the p-value in tukey and nemenyi test.*/
    private StudentizedRangeDistribution qDistribution 

    /* The distribution used to calculate the p-value in nemenyi test. */
    private ClassificationDistribution x2Distribution

    /* The distribution used to calculate the p-value in dunn test. */
    private NormalDistribution zDistribution

    private Array <CompareMeansResult> results = undefined              // Results for every source
    private HashTable<text, Array<CompareMeansResult>> sourceResults    // Results for each source

    /* Set if the test has already been calculated, and we simply want to run a post hoc follow up */
    private CompareMeansResult testResult = undefined
    private boolean defaultVarianceAssumption = true        
    private boolean defaultDistributionAssumption = true           

    Math math

    action Calculate(DataFrame frame)    
        CompareMeans compare
        compare:Ranked(Ranked())
        compare:Paired(Paired())
        compare:RepeatedMeasures(RepeatedMeasures())
        if not defaultVarianceAssumption
            compare:AssumeEqualVariances(AssumeEqualVariances())
        end
        if not defaultDistributionAssumption
            compare:AssumeNormalDistribution(AssumeNormalDistribution())
        end
        compare:SetExperimentalDesign(GetDesign())
        compare:Calculate(frame)
        SetExperimentalDesign(compare:GetDesign())
        Calculate(compare:GetResult())
    end

    private action RunTest(DataFrame frame)
    end

    action Calculate(CompareMeansResult result)
        if result = undefined
            alert("Prior CompareMeans test result is undefined.")
        end
        if result:GetGroupsTable():GetSize() > 0
            alert("Multivariate CompareMeansPairwise tests are not available yet.")
        end
        if result:GetGroupsFrame():GetSize() < 2
            alert("Prior CompareMeans test result must be from a test with at least two samples, your test only has "+
                   result:GetGroupsFrame():GetSize()+" samples. This doesn't make sense for a pairwise test.")
        end
        results = undefined
        sourceResults:Empty()

        testResult = result
        Ranked(testResult:IsRanked())
        RepeatedMeasures(testResult:IsRepeated())
        AssumeEqualVariances(testResult:HasEqualVariances())
        if GetDesign():GetDesignFrame() = undefined
            alert("Undefined ExperimentalDesign. Use the design from your CompareMeans test with the following action SetExperimentalDesign()")
        end
    
        if UsingFittedApproach()
            if Ranked()
                // Nemenyi (Lenient)
                // Dunn + Bonferroni (Strict)
                // Conover + Bonferroni (Strict)  
                // Dunn (None)
                // Conover (None)                
                CalculateRankedFittedApproach()
            else
                if testResult:GetGroupsTable():GetSize() > 0 // Multivariate: Not implemented yet
                    // Tukey HSD, Tukey-Kramer, Games-Howell (Lenient)
                    // EMMEANS + Bonferroni (Strict)
                    // EMMEANS (None)
                    CalculateMultivariateFittedApproach()
                else
                    // Tukey HSD, Tukey-Kramer, Games-Howell (Lenient)
                    // EMMEANS + Bonferroni (Strict)
                    // EMMEANS (None)
                    CalculateUnivariateFittedApproach()
                end
            end
        elseif UsingUnfittedApproach()
            if testResult:GetGroupsTable():GetSize() > 0 // Multivariate: Not implemented yet
                // Hotellings T2-test + Bonferroni (Strict)
                // Hotellings T2-test (None)
                CalculateMultivariateUnfittedApproach()
            else
                // Pairwise two-sample test + Bonferroni (Strict)
                // Pairwise two-sample test (None)
                CalculateUnivariateUnfittedApproach()
            end
        end                    
    end

    action AssumeEqualVariances(boolean assume)
        defaultVarianceAssumption = false
        parent:CompareMeans:AssumeEqualVariances(assume)
    end

    action AssumeNormalDistribution(boolean assume)
        defaultDistributionAssumption = false
        parent:CompareMeans:AssumeNormalDistribution(assume)
    end

    /*
        Attribute: Returns an array of all the CompareMeansResult objects
        Attribute: Example

        use Libraries.Compute.Statistics.DataFrame
        use Libraries.Compute.Statistics.Tests.CompareMeansPairwise
    
        DataFrame frame
        frame:Load("Data/Data.csv")
    
        ExperimentalDesign design
        design:AddBetweenSubjectsFactor("Age")
        design:AddBetweenSubjectsFactor("Group")
        design:AddDependentVariable("Response")
    
        CompareMeansPairwise compare
        compare:SetExperimentalDesign(design)
        frame:Calculate(compare)

        Array<CompareMeansResult> results = compare:GetResults()
    */
    action GetResults returns Array<CompareMeansResult>
        if testResult = undefined
            alert("It doesn't look like you've run the test yet. Use Calculate() to run the CompareMeansPairwise test.")
        end
        if results = undefined
            results = CompileResults()
        end
        return results
    end

    /*
        Attribute: Returns an array of the CompareMeansResult objects for a 
        single source (i.e. a main effect or an interaction effect)
        Attribute: Example

        use Libraries.Compute.Statistics.DataFrame
        use Libraries.Compute.Statistics.Tests.CompareMeansPairwise
    
        DataFrame frame
        frame:Load("Data/Data.csv")

        ExperimentalDesign design
        design:AddBetweenSubjectsFactor("Age")
        design:AddBetweenSubjectsFactor("Group")
        design:AddDependentVariable("Response")
    
        CompareMeansPairwise compare
        compare:SetExperimentalDesign(design)
        frame:Calculate(compare)

        Array<CompareMeansResult> results = compare:GetResults("Age")
    */
    action GetResults(text source) returns Array<CompareMeansResult>
        if testResult = undefined
            alert("It doesn't look like you've run the test yet. Use Calculate() to run the CompareMeansPairwise test.")
        end
        if sourceResults:HasKey(source)
            return sourceResults:GetValue(source)
        end
        alert("There are no results for that source. Try GetSources() to see the available effects.")
    end


    /*
        This returns the simple pairwise summary of the results for all effects.

        Attribute: Returns the pairwise summary.
        Attribute: Example

        use Libraries.Compute.Statistics.DataFrame
        use Libraries.Compute.Statistics.Tests.CompareMeansPairwise
    
        DataFrame frame
        frame:Load("Data/Data.csv")
    
        ExperimentalDesign design
        design:AddBetweenSubjectsFactor("Age")
        design:AddBetweenSubjectsFactor("Group")
        design:AddDependentVariable("Response")
    
        CompareMeansPairwise compare
        compare:SetExperimentalDesign(design)
        frame:Calculate(compare)

        output compare:GetPairwiseSummary() 
    */
    action GetPairwiseSummary returns text
        if testResult = undefined
            alert("It doesn't look like you've run the test yet. Use Calculate() to run the CompareMeansPairwise test.")
        end
        text summary = ""
        text lf = summary:GetLineFeed()
        integer digits = GetStatisticalFormatting():GetSignificantDigits()
        Array<CompareMeansResult> results = GetResults()
        integer i = 0
        repeat while i < results:GetSize()
            CompareMeansResult pair = results:Get(i)
            if i = 0
                summary = summary + pair:GetFormalTestName()
            end
            if pair:GetGroupsFrame():GetSize() > 1
                text group1 = pair:GetGroupsFrame():GetColumn(0):GetHeader()
                text group2 = pair:GetGroupsFrame():GetColumn(1):GetHeader()
                summary = summary + lf + "  " + group1 + " - " + group2 + ": p = " + math:Round(pair:GetProbabilityValue(), digits)
                if pair:IsSignificant()
                    summary = summary + " ** significant **"
                end
            end
            i = i + 1
        end
        return summary
    end

    /*
        This returns the simple pairwise summary of the results for a given effect.

        Attribute: Returns the pairwise summary.
        Attribute: Example

        use Libraries.Compute.Statistics.DataFrame
        use Libraries.Compute.Statistics.Tests.CompareMeansPairwise
    
        DataFrame frame
        frame:Load("Data/Data.csv")
    
        ExperimentalDesign design
        design:AddBetweenSubjectsFactor("Age")
        design:AddBetweenSubjectsFactor("Group")
        design:AddDependentVariable("Response")
    
        CompareMeansPairwise compare
        compare:SetExperimentalDesign(design)
        frame:Calculate(compare)

        output compare:GetPairwiseSummary("Age")
    */
    action GetPairwiseSummary(text source) returns text
        if testResult = undefined
            alert("It doesn't look like you've run the test yet. Use Calculate() to run the CompareMeansPairwise test.")
        end
        text summary = ""
        text lf = summary:GetLineFeed()
        integer digits = GetStatisticalFormatting():GetSignificantDigits()
        Array<CompareMeansResult> results = GetResults(source)
        integer i = 0
        repeat while i < results:GetSize()
            CompareMeansResult pair = results:Get(i)
            if i = 0
                summary = summary + pair:GetFormalTestName()
            end
            if pair:GetGroupsFrame():GetSize() > 1
                text group1 = pair:GetGroupsFrame():GetColumn(0):GetHeader()
                text group2 = pair:GetGroupsFrame():GetColumn(1):GetHeader()
                summary = summary + lf + "  " + group1 + " - " + group2 + ": p = " + math:Round(pair:GetProbabilityValue(), digits)
                if pair:IsSignificant()
                    summary = summary + " ** significant **"
                end
            end
            i = i + 1
        end
        return summary
    end

    action GetSources returns Array<text>
        if testResult = undefined
            alert("It doesn't look like you've run the test yet. Use Calculate() to run the CompareMeansPairwise test.")
        end
        return testResult:GetSources()
    end

    private action CalculateRankedFittedApproach()
        text source = ""
        Array<text> sources = testResult:GetSources()
        DataFrame rankedframe = GetDesign():GetRankedFrame():Copy()
        i = 0
        repeat while i < sources:GetSize()
            source = sources:Get(i)
            
            if testResult:GetInformation():GetValue(source):HasKey("tie correction sum")
                number tieCorrectionSum = testResult:GetInformation():GetValue(source):GetValue("tie correction sum")
                number variance = 0
                if testResult:GetInformation():GetValue(source):HasKey("variance")
                    variance = testResult:GetInformation():GetValue(source):GetValue("variance")
                end
                number testStatistic = testResult:GetTestStatistic()

                // Factor the groups by only this source/effect, ignoring other factors.
                rankedframe:EmptySelectedFactors()
                Array<text> factors = source:Split(":")
                j = 0
                repeat while j < factors:GetSize()
                    rankedframe:AddSelectedFactors(factors:Get(j))
                    j = j + 1
                end
                rankedframe:AddSelectedColumns(GetDesign():GetDependentVariables():Get(0))
                DataFrame sourceFrame = rankedframe:CreateNewDataFrameFromFactoredColumns()     

                // Apply multiple comparison test to this source/effect            
                sourceResults:Add(source, CalculatePairwiseUsingFittedApproachRanked(sourceFrame, testStatistic, tieCorrectionSum, variance))
            end
            i = i + 1
        end 
    end

    private action CalculateUnivariateFittedApproach()
        text source = ""
        Array<text> sources = testResult:GetSources()
        DataFrame frame = GetDesign():GetDesignFrame():Copy()
        i = 0
        repeat while i < sources:GetSize()
            source = sources:Get(i)
            number degreesOfFreedom = 0
            number meanSquaredError = 0 
            if testResult:GetInformation():GetValue(source):HasKey("error df")
                // Get error df from the anova for this source/effect and correct if necessary
                // This is only necessary if both the effect and the sphericity for that effect were significant
                number dfCorrection = 1
                if testResult:GetInformation():GetValue(source):HasKey("sphericity p") 
                    if testResult:GetInformation():GetValue(source):GetValue("p") < GetSignificanceLevel()
                        if testResult:GetInformation():GetValue(source):GetValue("sphericity p")  < GetSignificanceLevel()
                            dfCorrection = testResult:GetInformation():GetValue(source):GetValue("gg")
                        end
                    end
                end
                degreesOfFreedom = testResult:GetInformation():GetValue(source):GetValue("error df") * dfCorrection

                // Get mean squared error from the anova for this source/effect
                if testResult:GetInformation():GetValue(sources:Get(i)):HasKey("error ss")
                    meanSquaredError = testResult:GetInformation():GetValue(source):GetValue("error ss") / degreesOfFreedom
                end 

                // Factor the groups by only this source/effect, ignoring other factors.
                frame:EmptySelectedColumns()
                frame:EmptySelectedFactors()
                Array<text> factors = source:Split(":")
                j = 0
                repeat while j < factors:GetSize()
                    frame:AddSelectedFactors(factors:Get(j))
                    j = j + 1
                end
                frame:AddSelectedColumns(GetDesign():GetDependentVariables():Get(0))
                DataFrame sourceFrame = frame:CreateNewDataFrameFromFactoredColumns()     

                // Apply multiple comparison test to this source/effect            
                sourceResults:Add(source, CalculatePairwiseUsingFittedApproach(sourceFrame, meanSquaredError, degreesOfFreedom, dfCorrection))
            end
            i = i + 1
        end 
    end

    private action CalculateMultivariateFittedApproach()
    end

    private action CalculateUnivariateUnfittedApproach()
        text source = ""
        Array<text> sources = testResult:GetSources()
        DataFrame frame = GetDesign():GetDesignFrame():Copy()
        i = 0
        repeat while i < sources:GetSize()
            source = sources:Get(i)
            if source not= GetDesign():GetSubjectIdentifier() and source not= "Residual Error"  
                number dfCorrection = 1
                if testResult:GetInformation():GetValue(source):HasKey("sphericity p") 
                    if testResult:GetInformation():GetValue(source):GetValue("p") < GetSignificanceLevel()
                        if testResult:GetInformation():GetValue(source):GetValue("sphericity p")  < GetSignificanceLevel()
                            dfCorrection = testResult:GetInformation():GetValue(source):GetValue("gg")
                        end
                    end
                end
    
                // Factor the groups by only this source/effect, ignoring other factors.
                frame:EmptySelectedColumns()
                frame:EmptySelectedFactors()
                Array<text> factors = source:Split(":")
                j = 0
                repeat while j < factors:GetSize()
                    frame:AddSelectedFactors(factors:Get(j))
                    j = j + 1
                end
                frame:AddSelectedColumns(GetDesign():GetDependentVariables():Get(0))
                DataFrame sourceFrame = frame:CreateNewDataFrameFromFactoredColumns() 
    
                // Apply multiple comparison test to this source/effect           
                sourceResults:Add(source, CalculatePairwiseUsingUnfittedApproach(sourceFrame, source, dfCorrection))
            end
            i = i + 1
        end
    end

    private action CalculateMultivariateUnfittedApproach()
    end

    private action CalculatePairwiseUsingFittedApproach(DataFrame sourceFrame, number standardError, number degreesOfFreedom, number dfCorrection) returns Array<CompareMeansResult>
        integer numberOfGroups = sourceFrame:GetSize()
        number numberOfTestsPerformed = (numberOfGroups * (numberOfGroups-1)) / 2 // numberOfGroups choose 2

        sourceFrame:SelectAllColumns()
        Array<Variance> vars = sourceFrame:VarianceSelectedColumns()

        Array<CompareMeansResult> sourceResults
        i = 0
        repeat while i < numberOfGroups
            j = i + 1
            repeat while j < numberOfGroups
                DataFrameColumn left = sourceFrame:GetColumn(i)
                DataFrameColumn right = sourceFrame:GetColumn(j)
                text group1Text = left:GetHeader()
                text group2Text = right:GetHeader()
                text combo = group1Text+"_"+group2Text

                DataFrame pairFrame
                pairFrame:AddColumn(left)
                pairFrame:AddColumn(right)

                number meanL = vars:Get(i):GetMean()
                number varL = vars:Get(i):GetVariance()

                number meanR = vars:Get(j):GetMean()
                number varR = vars:Get(j):GetVariance()

                number sizeL = left:GetSize() - left:GetUndefinedSize()
                number sizeR = right:GetSize() - right:GetUndefinedSize()

                CompareMeansResult pair
                pair:SetSignificanceLevel(GetSignificanceLevel())
                pair:SetFormat(GetStatisticalFormatting())
                pair:SetGroupsFrame(pairFrame)
                pair:NormalDistribution(true)
                pair:EqualVariances(AssumeEqualVariances() or defaultVarianceAssumption)

                text dfCorrectionName = ""
                text pCorrectionName = " with Bonferroni Correction"

                if dfCorrection not= 1
                    dfCorrectionName = " with Greenhouse-Geisser Correction"
                end

                number t = 0
                number p = 0
                if UsingLenientCorrection()
                    number error = 0
                    number q = 0
                    if standardError = 0
                        // Games-Howell - uses Welch's df and pooled standard error instead of mse and error df
                        number seL = varL / sizeL
                        number seR = varR / sizeR
                        degreesOfFreedom = ((seL + seR) * (seL + seR)) / ((seL * seL / (sizeL - 1)) + (seR * seR / (sizeR - 1)))
                        error = math:SquareRoot(seL + seR)
                        t = (meanL - meanR) / error
                        q = math:SquareRoot(2) * math:AbsoluteValue(t)
                        qDistribution:Setup(numberOfGroups, degreesOfFreedom)
                        p = (1 - qDistribution:CumulativeDistribution(q))
                        pair:SetFormalTestName("Games-Howell Multiple Comparisons Test with Welch Correction") 
                    else
                        if sizeL = sizeR
                            // Tukey HSD 
                            error = math:SquareRoot(standardError * (2.0/sizeL))
                            t = (meanL - meanR) / error
                            q = math:SquareRoot(2) * math:AbsoluteValue(t)
                        else
                            // Tukey-Kramer 
                            error = math:SquareRoot(standardError * (1.0 / sizeL + 1.0 / sizeR))
                            t = (meanL - meanR) / error
                            q = math:SquareRoot(2) * math:AbsoluteValue(t)
                        end
                        qDistribution:Setup(numberOfGroups, degreesOfFreedom)
                        p = (1 - qDistribution:CumulativeDistribution(q))
                        pair:SetFormalTestName("Tukey HSD Multiple Comparisons Test" + dfCorrectionName)   
                    end
                else
                    if standardError = 0
                        // Welch's df and pooled standard error instead of mse and error df
                        number seL = varL / sizeL
                        number seR = varR / sizeR
                        degreesOfFreedom = ((seL + seR) * (seL + seR)) / ((seL * seL / (sizeL - 1)) + (seR * seR / (sizeR - 1)))
                        error = math:SquareRoot(seL + seR)
                        t = (meanL - meanR) / error
                        tDistribution:Setup(degreesOfFreedom)
                        p = 2.0 * tDistribution:CumulativeDistribution(-math:AbsoluteValue(t))
                        dfCorrectionName = " with Welch Correction"
                        pCorrectionName = " and Bonferroni Correction"
                    else
                        t = (meanL - meanR) / math:SquareRoot(standardError * (1.0 / sizeL + 1.0 / sizeR))
                        tDistribution:Setup(degreesOfFreedom)
                        p = 2.0 * tDistribution:CumulativeDistribution(-math:AbsoluteValue(t))
                    end

                    if UsingStrictCorrection()
                        // Adjust p-value using Bonferroni correction
                        p = p * numberOfTestsPerformed 
                        if p > 1
                            p = 1
                        end
                        pair:SetFormalTestName("Estimated Marginal Means Comparisons" + dfCorrectionName + pCorrectionName)
                    else
                        pair:SetFormalTestName("Estimated Marginal Means Comparisons" + dfCorrectionName)
                    end
                end

                // Save the result
                pair:SetTestStatistic(combo, "t", t)
                pair:SetProbabilityValue(combo, "t", p)
                pair:SetDegreesOfFreedom(combo, "t", degreesOfFreedom)
                pair:SetInformation(combo,"t", t)
                pair:SetInformation(combo,"df", degreesOfFreedom)
                pair:SetInformation(combo,"p", p)
                sourceResults:Add(pair)
                j = j + 1
            end
            i = i + 1
        end
        return sourceResults
    end

    private action CalculatePairwiseUsingFittedApproachRanked(DataFrame sourceFrame, number testStatistic, number tieCorrectionSum, number testVariance) returns Array<CompareMeansResult>
        number numberOfGroups = sourceFrame:GetSize()  
        number numberOfSubjects = GetDesign():GetNumberOfSubjects()
        number numberOfTestsPerformed = (numberOfGroups * (numberOfGroups-1)) / 2 // numberOfGroups choose 2

        sourceFrame:SelectAllColumns()
        Array<Summarize> sourceSummaries = sourceFrame:SummarizeSelectedColumns()

        Array<CompareMeansResult> sourceResults
        i = 0
        repeat while i < numberOfGroups
            j = i + 1
            repeat while j < numberOfGroups
                DataFrameColumn left = sourceFrame:GetColumn(i)
                DataFrameColumn right = sourceFrame:GetColumn(j)
                text group1Text = left:GetHeader()
                text group2Text = right:GetHeader()
                text combo = group1Text+"_"+group2Text

                DataFrame pairFrame
                pairFrame:AddColumn(left)
                pairFrame:AddColumn(right)

                number sumRankL = sourceSummaries:Get(i):GetSum()
                number sumRankR = sourceSummaries:Get(j):GetSum() 

                number meanRankL = sourceSummaries:Get(i):GetMean()
                number meanRankR = sourceSummaries:Get(j):GetMean()              

                number sizeL = left:GetSize() - left:GetUndefinedSize()
                number sizeR = right:GetSize() - right:GetUndefinedSize()

                CompareMeansResult pair
                pair:SetSignificanceLevel(GetSignificanceLevel())
                pair:SetFormat(GetStatisticalFormatting())
                pair:SetGroupsFrame(pairFrame)
                pair:NormalDistribution(false)
                pair:EqualVariances(AssumeEqualVariances() or defaultVarianceAssumption)

                number t = 0
                number df = 0
                number p = 0
                if UsingLenientCorrection()
                    // https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf
                    // https://arxiv.org/pdf/2202.09131.pdf
                    // https://www.researchgate.net/publication/289506762_Overview_of_Friedman's_Test_and_Post-hoc_Analysis
                    // https://dl.icdst.org/pdfs/files3/22a131fac452ed75639ed5b0680761ac.pdf
                    // https://cran.r-hub.io/web/packages/PMCMR/vignettes/PMCMR.pdf
                    // https://www.osti.gov/servlets/purl/6057803

                    // Nemenyi Multiple Comparisons Test --> corrects for familywise error
                    if RepeatedMeasures()
                        // Nemenyi Repeated Measures - ties don't matter after friedman's test
                        number error = math:SquareRoot(numberOfGroups * (numberOfGroups + 1.0) / (6.0 * numberOfSubjects))                        
                        number difference = (meanRankL-meanRankR)
                        t = difference / error
                        number q = math:AbsoluteValue(t) * math:SquareRoot(2)
                        df = numberOfGroups
                        qDistribution:Setup(df, q:GetPositiveInfinityValue())
                        p = 1 - qDistribution:CumulativeDistribution(q)
                        pair:SetFormalTestName("Nemenyi-Tukey Multiple Comparisons Test")
                    else 
                        // If ties are present, use chi squared distribution.
                        if tieCorrectionSum > 0 
                            // Nemenyi (Chi-Squared Approximation) should be: sizeL >= 6 and sizeR >= 6 and numberOfGroups >= 4 
                            number error = math:SquareRoot(((numberOfSubjects * (numberOfSubjects + 1)) / 12.0) * (1.0 / sizeL + 1.0 / sizeR))
                            number difference = (meanRankL-meanRankR)
                            t = difference / error
                            number tieCorrection = 1 - (tieCorrectionSum / (numberOfSubjects * numberOfSubjects * numberOfSubjects - numberOfSubjects))
                            t = t / tieCorrection
                            number x2 = math:AbsoluteValue(t*t)
                            df = numberOfGroups - 1.0
                            x2Distribution:Setup(df)
                            p = 1 - x2Distribution:CumulativeDistribution(x2) 
                            pair:SetFormalTestName("Nemenyi Multiple Comparisons Test with tie correction")
                            //  alert("Only accurate to about the second or third decimal when comparing to R.")  
                            //  alert("NEEDS WORK.") 
                        else
                            // Nemenyi (Tukey-Kramer Approximation)
                            number error = math:SquareRoot(((numberOfSubjects * (numberOfSubjects + 1)) / 12.0) * (1.0 / sizeL + 1.0 / sizeR))
                            number difference = (meanRankL-meanRankR)
                            t = difference / error
                            number q = math:AbsoluteValue(t) * math:SquareRoot(2)
                            df = numberOfGroups
                            qDistribution:Setup(df, q:GetPositiveInfinityValue())
                            p = 1 - qDistribution:CumulativeDistribution(q)
                            pair:SetFormalTestName("Nemenyi-Tukey-Kramer Multiple Comparisons Test")
                        end
                       
                    end                     
                else
                    // Dunn Multiple Comparisons Test           --> does not correct for familywise error
                    // Conover-Iman Multiple Comparisons Test   --> does not correct for familywise error
                    if RepeatedMeasures()
                        if AssumeEqualVariances()
                            // All of the following sources produce the same result. R's PMCMRplus package is incorrect as of 11/14/23.
                            // CHOSEN: See page 12: https://www.osti.gov/servlets/purl/6057803
                            // See: https://real-statistics.com/anova-repeated-measures/friedman-test/friedman-test-post-hoc-analysis/
                            // See page 4: https://arxiv.org/pdf/2202.09131.pdf
                            // See: http://140.117.153.69/ctdr/files/857_1734.pdf
                            // See page 177: http://140.117.153.69/ctdr/files/857_1734.pdf
                            // See: https://juangvillegas.files.wordpress.com/2011/08/friedman-test-24062011.pdf

                            // Conover-Iman
                            // Verified calculations in Conover (1979) and Conover (1999)
                            text tieCorrectionName = ""
                            text pCorrectionName = " with Bonferroni Correction"
                            df = (numberOfSubjects - 1.0) * (numberOfGroups - 1.0)
                            number s2 = numberOfGroups * numberOfSubjects * (numberOfGroups + 1.0) / 12.0
                            if tieCorrectionSum > 0 
                                s2 = testVariance
                                tieCorrectionName = " with tie correction" 
                                pCorrectionName = " and Bonferroni Correction"
                            end   
                            number temp = 1.0 - (testStatistic / (numberOfSubjects * (numberOfGroups - 1.0)))
                            number error = math:SquareRoot(s2 * (2 * numberOfSubjects * (numberOfGroups - 1.0) / df) * temp)
                            number difference = (sumRankL - sumRankR)
                            t = difference / error
                            tDistribution:Setup(df)
                            p = 2.0 * tDistribution:CumulativeDistribution(-math:AbsoluteValue(t))
        
                            if UsingStrictCorrection()
                                // Adjust p-value using Bonferroni correction
                                p = p * numberOfTestsPerformed 
                                if p > 1
                                    p = 1
                                end
                                pair:SetFormalTestName("Conover-Iman Multiple Comparisons Test with Bonferroni Correction")
                            else
                                pair:SetFormalTestName("Conover-Iman Multiple Comparisons Test")
                            end 
                        else
                            // See page 686: https://dl.icdst.org/pdfs/files3/22a131fac452ed75639ed5b0680761ac.pdf
                            // See page 12: https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf
                            // See: https://manuals.pqstat.pl/en:statpqpl:porown3grpl:nparpl:anova_friepl
                            // Dunn-Bonferroni     
                            number error = math:SquareRoot(numberOfGroups * (numberOfGroups + 1.0) / (6.0 * numberOfSubjects))
                            number difference = (meanRankL-meanRankR)
                            t = difference / error
                            number z = math:AbsoluteValue(t)
                            p = 2.0 * (1.0 - zDistribution:CumulativeDistribution(z))
        
                            if UsingStrictCorrection()
                                // Adjust p-value using Bonferroni correction
                                p = p * numberOfTestsPerformed 
                                if p > 1
                                    p = 1
                                end
                                pair:SetFormalTestName("Dunn Multiple Comparisons Test with Bonferroni Correction")
                            else
                                pair:SetFormalTestName("Dunn Multiple Comparisons Test")
                            end   
                            //  alert("NEEDS WORK.") 
                        end                  
                    else
                        if AssumeEqualVariances()
                            // See: https://real-statistics.com/one-way-analysis-of-variance-anova/kruskal-wallis-test/conover-test-after-kw/
                            // See page 11: https://www.osti.gov/servlets/purl/6057803
                            // Conover-Iman
                            text tieCorrectionName = ""
                            text pCorrectionName = " with Bonferroni Correction"

                            df = (numberOfSubjects - numberOfGroups)  
                            number s2 = (numberOfSubjects * (numberOfSubjects + 1)) / 12.0
                            if tieCorrectionSum > 0 
                                s2 = testVariance
                                tieCorrectionName = " with tie correction" 
                                pCorrectionName = " and Bonferroni Correction"
                            end
                            number temp = (numberOfSubjects - 1.0 - testStatistic) / df
                            number error = math:SquareRoot(s2 * temp * (1.0 / sizeL + 1.0 / sizeR))
                            number difference = (meanRankL-meanRankR)
                            t = difference / error
                            tDistribution:Setup(df)
                            p = 2.0 * tDistribution:CumulativeDistribution(-math:AbsoluteValue(t))

                            if UsingStrictCorrection()
                                // Adjust p-value using Bonferroni correction
                                p = p * numberOfTestsPerformed 
                                if p > 1
                                    p = 1
                                end
                                pair:SetFormalTestName("Conover-Iman Multiple Comparisons Test" + tieCorrectionName + pCorrectionName)
                            else
                                pair:SetFormalTestName("Conover-Iman Multiple Comparisons Test" + tieCorrectionName)
                            end
                        else
                            // See page 298: https://journals-sagepub-com.ezproxy.library.unlv.edu/doi/pdf/10.1177/1536867X1501500117
                            // See page 614: https://dl.icdst.org/pdfs/files3/22a131fac452ed75639ed5b0680761ac.pdf
                            // Dunn-Bonferroni 
                            text tieCorrectionName = ""
                            text pCorrectionName = " with Bonferroni Correction"
    
                            number tieCorrection = 0
                            if tieCorrectionSum > 0 
                                tieCorrection = tieCorrectionSum / (12.0 * (numberOfSubjects - 1.0))
                                tieCorrectionName = " with tie correction" 
                                pCorrectionName = " and Bonferroni Correction"
                            end       
                            number error = math:SquareRoot(((numberOfSubjects * (numberOfSubjects + 1)) / 12.0 - tieCorrection) * (1.0 / sizeL + 1.0 / sizeR))
                            number difference = (meanRankL-meanRankR)
                            t = difference / error
                            number z = math:AbsoluteValue(t)
                            p = 2.0 * (1.0 - zDistribution:CumulativeDistribution(z))
        
                            if UsingStrictCorrection()
                                // Adjust p-value using Bonferroni correction
                                p = p * numberOfTestsPerformed 
                                if p > 1
                                    p = 1
                                end
                                pair:SetFormalTestName("Dunn Multiple Comparisons Test" + tieCorrectionName + pCorrectionName)
                            else
                                pair:SetFormalTestName("Dunn Multiple Comparisons Test" + tieCorrectionName)
                            end
                        end
                    end
                end

                // Save the result
                pair:SetTestStatistic(combo, "t", t)
                pair:SetProbabilityValue(combo, "t", p)
                pair:SetDegreesOfFreedom(combo, "t", df)
                pair:SetInformation(combo,"t", t)
                pair:SetInformation(combo,"df", df)
                pair:SetInformation(combo,"p", p)
                sourceResults:Add(pair)
                j = j + 1
            end
            i = i + 1
        end
        return sourceResults
    end

    private action CalculatePairwiseUsingUnfittedApproach(DataFrame sourceFrame, text source, number dfCorrection) returns Array<CompareMeansResult>
        number numberOfGroups = sourceFrame:GetSize()
        number numberOfTestsPerformed = (numberOfGroups * (numberOfGroups-1)) / 2 // numberOfGroups choose 2

        // See: https://www.statisticshowto.com/pooled-standard-deviation/
        number pooledSD = 0
        number pooledDF = 0
        if not Ranked() and AssumeEqualVariances()
            sourceFrame:SelectAllColumns()
            Array<Summarize> summaries = sourceFrame:SummarizeSelectedColumns()
            number top = 0
            number bottom = 0
            i = 0
            repeat while i < summaries:GetSize()
                number n = summaries:Get(i):GetSize()
                number var = summaries:Get(i):GetVariance()
                top = top + (n - 1) * var
                bottom = bottom + (n - 1)
                i = i + 1
            end
            pooledSD = math:SquareRoot(top / bottom)
            pooledDF = bottom
        end

        Array<CompareMeansResult> sourceResults
        i = 0
        repeat while i < numberOfGroups
            j = i + 1
            repeat while j < numberOfGroups
                DataFrameColumn left = sourceFrame:GetColumn(i)
                DataFrameColumn right = sourceFrame:GetColumn(j)
                text group1Text = left:GetHeader()
                text group2Text = right:GetHeader()
                text combo = group1Text+"_"+group2Text

                DataFrame pairFrame
                pairFrame:AddColumn(left)
                pairFrame:AddColumn(right)
                pairFrame:SelectAllColumns()

                // If the source is within-subjects or if the two samples are from the same subjects but different within-subjects levels. 
                boolean paired = CheckIfPaired(source, group1Text, group2Text)

                // Run the appropriate two-sample test to get the statistic
                CompareMeans compare
                compare:Paired(paired)
                compare:AssumeEqualVariances(AssumeEqualVariances())
                compare:Ranked(Ranked())
                compare:Calculate(pairFrame)
                CompareMeansResult result = compare:GetResult()                 

                text pooled = ""
                number stat = result:GetTestStatistic()    
                number df = 0
                if not Ranked()
                    df = result:GetDegreesOfFreedom()
                end
                number p = result:GetProbabilityValue() 
                if not Ranked() and not paired and AssumeEqualVariances() // pooled sd only for equal-variance t-test
                    number m1 = result:GetInformation():GetValue(group1Text):GetValue("mean")
                    number m2 = result:GetInformation():GetValue(group2Text):GetValue("mean")
                    number n1 = result:GetInformation():GetValue(group1Text):GetValue("size")
                    number n2 = result:GetInformation():GetValue(group2Text):GetValue("size")
                    df = pooledDF
                    number error = pooledSD * math:SquareRoot(1 / n1 + 1 / n2)
                    stat = (m1 - m2) / error
                    tDistribution:Setup(df)
                    p = 2.0 * tDistribution:CumulativeDistribution(-math:AbsoluteValue(stat))
                    pooled = " and pooled standard deviation"
                end

                text testName = ""
                if UsingStrictCorrection()
                    // Adjust p-value using Bonferroni correction
                    p = p * numberOfTestsPerformed 
                    if p > 1
                        p = 1
                    end
                    testName = "Pairwise " + result:GetFormalTestName() + " with Bonferroni Correction" + pooled
                else
                    if pooled not= ""
                        pooled = " with pooled standard deviation"
                    end
                    testName = "Pairwise " + result:GetFormalTestName() + pooled
                end

                CompareMeansResult pair
                pair:SetSignificanceLevel(GetSignificanceLevel())
                pair:SetFormat(GetStatisticalFormatting())
                pair:SetGroupsFrame(pairFrame)
                pair:SetFormalTestName(testName)
                pair:SetTestStatistic(combo, result:GetTestStatisticName(), stat)
                pair:SetProbabilityValue(combo, result:GetTestStatisticName(), p)
                pair:SetDegreesOfFreedom(combo, result:GetTestStatisticName(), df)
                pair:SetInformation(combo,result:GetTestStatisticName(), stat)
                pair:SetInformation(combo,"df", df)
                pair:SetInformation(combo,"p", p)
                sourceResults:Add(pair)
                j = j + 1
            end
            i = i + 1
        end
        return sourceResults
    end

    private action CheckIfPaired(text source, text sample1, text sample2) returns boolean
        Array<text> factors = source:Split(":")
        Array<text> betweenFactors = GetDesign():GetBetweenSubjectsFactors()

        Text sample1Text
        sample1Text:SetValue(sample1)
        Array<text> sample1Levels = sample1Text:Split(".")

        Text sample2Text
        sample2Text:SetValue(sample2)
        Array<text> sample2Levels = sample2Text:Split(".")
        
        // If factor is a between-subjects and the same subjects are used then use paired, otherwise use independent
        boolean sameBetween = true
        i = 0
        repeat while i < factors:GetSize()
            j = 0
            repeat while j < betweenFactors:GetSize()
                if factors:Get(i) = betweenFactors:Get(j)
                    if sample1Levels:Get(i) not= sample2Levels:Get(i)
                        sameBetween = false
                    end
                end
                j = j + 1
            end
            i = i + 1
        end
        return sameBetween
    end

    private action CompileResults returns Array<CompareMeansResult>
        Array<CompareMeansResult> results
        Array<text> sources = GetSources()
        i = 0
        repeat while i < sources:GetSize()
            if sourceResults:HasKey(sources:Get(i))
                Array<CompareMeansResult> sResults = sourceResults:GetValue(sources:Get(i))
                j = 0
                repeat while j < sResults:GetSize()
                    results:Add(sResults:Get(j))
                    j = j + 1
                end
            end
            i = i + 1
        end
        return results
    end

    /* 
    INFORMATION:

    Fitted approach:   
            This is the default, it uses pairwise estimated marginal means based on the Comparemeans several-sample test result.
    Unfitted approach:    
            This uses pairwise two-sample CompareMeans tests (i.e. t-test, wilcoxon test, etc)

    Corrections: 
        These are applied to the degrees of freedom or p-values after a standalone test to correct for familywise error.
    Multiple Comparison Tests:
        Standalone tests that sometimes inherently apply correction and sometimes need correction applied afterwards.    

    Lenient:
        Tukey HSD Multiple Comparison Test
            After CompareSeveralMeans
            Find which groups differ using 'Honest Significant Difference' to compare each mean with every other mean
            For more information: https://en.wikipedia.org/wiki/Tukey%27s_range_test

        Tukey-Kramer Multiple Comparison Test (Tukey Extension for non-equal sample sizes)
            After CompareSeveralMeans
            Find which groups differ using 'Honest Significant Difference' to compare each mean with every other mean
            For more information: https://en.wikipedia.org/wiki/Tukey%27s_range_test 

        Games-Howell Multiple Comparison Test (Tukey Extension for non-equal variances)
            After CompareSeveralMeans > AssumeEqualVariances(false)
            Find which groups differ using 'Honest Significant Difference' to compare each mean with every other mean
            For more information: https://en.wikipedia.org/wiki/Post_hoc_analysis      

        Nemenyi Multiple Comparison Test
            After CompareSeveralRankedMeans OR CompareSeveralRelatedRankedMeans
            For more information: https://en.wikipedia.org/wiki/Nemenyi_test

    Strict:
        Bonferroni Correction 
            After CompareSeveralMeans
            Controls for family-wise error rate by adjusting p-values for independent samples
            For more information: https://en.wikipedia.org/wiki/Bonferroni_correction

        Greenhouse-Geisser Correction 
            After CompareSeveralRelatedMeans
            Controls for family-wise error rate by adjusting degrees of freedom for dependent samples
            For more information: https://en.wikipedia.org/wiki/Greenhouse%E2%80%93Geisser_correction

        Huynhâ€“Feldt Correction 
            After CompareSeveralRelatedMeans
            Controls for family-wise error rate by adjusting degrees of freedom for dependent samples
            For more information: https://en.wikipedia.org/wiki/Huynh%E2%80%93Feldt_correction

        Conover-Iman-Bonferroni Multiple Comparison Test
            After CompareSeveralRankedMeans
            For more information: https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance
            For more information: https://en.wikipedia.org/wiki/Friedman_test
            For more information: https://en.wikipedia.org/wiki/Post_hoc_analysis

        Dunn-Bonferroni Multiple Comparison Test
            After CompareSeveralRankedMeans > AssumeEqualVariances(false)
            Find which groups differ between pairs of means when there are many rank ordered groups
            For more information: https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance
            For more information: https://en.wikipedia.org/wiki/Post_hoc_analysis

        ** NOT IMPLEMENTED YET **
        Benjamini-Hochberg Correction
            After CompareSeveralMeans OR CompareSeveralRelatedMeans
            For more information: https://en.wikipedia.org/wiki/False_discovery_rate

        ** NOT IMPLEMENTED YET **
        Holm-Bonferroni Correction (Bonferroni Extension)
            After CompareSeveralMeans OR CompareSeveralRelatedMeans
            Controls for family-wise error rate by adjusting p-values
            For more information: https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method

        ** NOT IMPLEMENTED YET **
        Sidak Correction 
            After CompareSeveralMeans 
            Controls for family-wise error rate by adjusting p-values
            For more information: https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction

    Extremely Strict:
        ** NOT IMPLEMENTED YET **
        ScheffÃ© Multiple Comparison Test
            After CompareSeveralMeans
            Controls for family-wise error rate by adjusting p-values
            For more information: https://en.wikipedia.org/wiki/Bonferroni_correction 
    */
end